{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import time\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import SubsetRandomSampler \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create single pose dataset for dnn model\n",
    "class SinglePoseDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, n_frames=5):\n",
    "        \n",
    "        # get data and label from csvget \n",
    "        dataset, labels = SinglePoseDataset.ReadPoseData(n_frames)\n",
    "        \n",
    "        self.X = dataset\n",
    "        self.y = labels\n",
    "    \n",
    "    # read dataset from csv file\n",
    "    @staticmethod\n",
    "    def ReadPoseData(n_frames):\n",
    "        \n",
    "        # get csv file path\n",
    "        curr_dir = os.getcwd()\n",
    "        csv_file_path = os.path.join(curr_dir, 'data/drop_res.csv')\n",
    "        \n",
    "        # list for storing data and labels\n",
    "        data  = []\n",
    "        label = []\n",
    "        \n",
    "        # lenth of sequence\n",
    "        #n_frames = SinglePoseDataset.n_frames\n",
    "        \n",
    "        # read csv file\n",
    "        KP_df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # convert pos_class to categories\n",
    "        KP_df['pos_class'] = KP_df['pos_class'].astype('category')\n",
    "        KP_df['pos_class'] = KP_df['pos_class'].cat.codes\n",
    "\n",
    "        # skipping (0-3) colomns , return values of all rows and columns from 4 to last\n",
    "        features = KP_df.iloc[:,5:].values\n",
    "        #return values of pose_class \n",
    "        pose_class = KP_df['pos_class'].values\n",
    "        # normalize keypoints \n",
    "        SinglePoseDataset.normalize_min_(features)\n",
    "        # append multiple rows to create a sequence of data\n",
    "        for i in range(features.shape[0]-n_frames):\n",
    "            data.append(features[i:i+n_frames,...])\n",
    "            label_sequence = pose_class[i:i+n_frames]\n",
    "            unique, counts = np.unique(label_sequence, return_counts=True)\n",
    "            label.append(unique[np.argmax(counts)])\n",
    "            \n",
    "        data , label =  np.array(data, dtype = np.float), np.array(label, dtype = np.int_)\n",
    "        \n",
    "        return data , label\n",
    "    \n",
    "    # min-max normalization to scale the x, y coordinates in range (0-1) \n",
    "    @staticmethod\n",
    "    def normalize_min_(pose:np.ndarray):\n",
    "        pose = pose.reshape(len(pose),-1,2)\n",
    "        for i in range(len(pose)):\n",
    "            xmin = np.min(pose[i,:,0]) \n",
    "            ymin = np.min(pose[i,:,1])\n",
    "            xlen = np.max(pose[i,:,0]) - xmin\n",
    "            ylen = np.max(pose[i,:,1]) - ymin\n",
    "\n",
    "            if(xlen==0): pose[i,:,0]=0\n",
    "            else:\n",
    "                pose[i,:,0] -= xmin \n",
    "                pose[i,:,0] /= xlen\n",
    "\n",
    "            if(ylen==0): pose[i,:,1]=0\n",
    "            else:\n",
    "                pose[i,:,1] -= ymin\n",
    "                pose[i,:,1] /= ylen\n",
    "        return pose\n",
    "    \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.X)\n",
    "        \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data  = torch.tensor(self.X[idx], dtype=torch.float) \n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        \n",
    "        return [data,label]\n",
    "    \n",
    "    # get indexes for train and test rows\n",
    "    \n",
    "    '''\n",
    "    def get_splits(self, n_test = 0.2, n_valid=0.2):\n",
    "        \n",
    "        # determine sizes \n",
    "        test_size = round(n_test * len(self.X))\n",
    "        valid_size = round(n_valid * len(self.X))\n",
    "        train_size = len(self.X)-(test_size+valid_size)\n",
    "        print(train_size, valid_size, test_size)\n",
    "        # calculate the split \n",
    "        return random_split(self, [train_size, valid_size, test_size])\n",
    "    '''\n",
    "    \n",
    "    def get_class_labels(self):\n",
    "        \n",
    "        labels = [\"Fall\",\"Stand\", \"Tie\"]\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def reshape_features(self):\n",
    "        self.X = self.X.reshape(-1, self.X.shape[1]*self.X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SinglePoseDataset(n_frames=20)\n",
    "targets = dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, valid_idx = train_test_split(np.arange(len(targets)), test_size=0.2, stratify=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(targets[train_idx], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(targets[val_idx], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(dataset.X, dataset.y, test_size=0.2, stratify=dataset.y)\n",
    "np.unique(y_train, return_counts=True)\n",
    "np.unique(y_val, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(X_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(val_idx)\n",
    "train_loader  = DataLoader(dataset, batch_size=32, sampler=train_sampler, drop_last=True)\n",
    "valid_loader  = DataLoader(dataset, batch_size=32, sampler=valid_sampler, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.sampler.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataset.y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips and tricks to training Fall Models\n",
    "* FallNet Architecture:\n",
    "* Input ---> (FC:1024 --> batch_norm --> ReLU --> Dropout)*2 + input --> output \n",
    "* Output ---> (FC:1024 --> batch_norm --> ReLU --> Dropout)*2 + output --> (FC:1042 --> FC:3)\n",
    "+ sign shows skip connections\n",
    "* To avoid overfitting use: dropout, batch normalization, weight regularization, increase size of training set for by performing some data augmentations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, class_num, initrange=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.input = input_dim\n",
    "        self.hidden_size = 24\n",
    "        self.rnn = nn.RNN(self.input, self.hidden_size, num_layers=5, dropout=0.3 , batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_size*5,160)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(160)\n",
    "        self.fc2 = torch.nn.Linear(160,160)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(160)\n",
    "        self.fc3 = torch.nn.Linear(160,80)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(80)\n",
    "        self.fc4 = torch.nn.Linear(80, class_num)\n",
    "        self.init_weights(initrange)\n",
    "        self.class_num = class_num\n",
    "    \n",
    "    def init_weights(self, initrange):\n",
    "        \n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "        self.fc3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc3.bias.data.zero_()\n",
    "        self.fc4.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc4.bias.data.zero_()\n",
    "    \n",
    "    \n",
    "    def forward(self, _input, batch_size):\n",
    "        \n",
    "        hidden_cell = torch.randn(5, batch_size, self.hidden_size, dtype=torch.float).to(device)\n",
    "        \n",
    "        rnn_feat, newh_cell= self.rnn(_input, hidden_cell)\n",
    "        rnn_feat = rnn_feat.reshape((-1,rnn_feat.shape[1]*rnn_feat.shape[2]))\n",
    "        \n",
    "        _fc1 = self.bn1(self.fc1(rnn_feat))\n",
    "        #_fc1 = F.dropout(F.relu(_fc1), p=0.3)\n",
    "        _fc1 = F.relu(_fc1)\n",
    "        \n",
    "        _fc2 = self.bn2(self.fc2(_fc1))\n",
    "        _fc2 = F.relu(_fc2)\n",
    "        \n",
    "        _fc3 = self.bn3(self.fc3(_fc2))\n",
    "        _fc3 = F.relu(_fc3)\n",
    "        \n",
    "        _fc4 = self.fc4(_fc3)\n",
    "        output = F.softmax(_fc4, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, class_num, initrange=0.5):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.input = input_dim\n",
    "        self.hidden_size = 24\n",
    "        self.rnn = nn.GRU(self.input, self.hidden_size, num_layers=5, dropout=0.3 , batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_size*5,160)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(160)\n",
    "        self.fc2 = torch.nn.Linear(160,160)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(160)\n",
    "        self.fc3 = torch.nn.Linear(160,80)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(80)\n",
    "        self.fc4 = torch.nn.Linear(80, class_num)\n",
    "        self.init_weights(initrange)\n",
    "        self.class_num = class_num\n",
    "    \n",
    "    def init_weights(self, initrange):\n",
    "        \n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "        self.fc3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc3.bias.data.zero_()\n",
    "        self.fc4.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc4.bias.data.zero_()\n",
    "    \n",
    "    \n",
    "    def forward(self, _input, batch_size):\n",
    "        \n",
    "        hidden_cell = torch.randn(5, batch_size, self.hidden_size, dtype=torch.float).to(device)\n",
    "        \n",
    "        rnn_feat, newh_cell= self.rnn(_input, hidden_cell)\n",
    "        rnn_feat = rnn_feat.reshape((-1,rnn_feat.shape[1]*rnn_feat.shape[2]))\n",
    "        \n",
    "        _fc1 = self.bn1(self.fc1(rnn_feat))\n",
    "        #_fc1 = F.dropout(F.relu(_fc1), p=0.3)\n",
    "        _fc1 = F.relu(_fc1)\n",
    "        \n",
    "        _fc2 = self.bn2(self.fc2(_fc1))\n",
    "        _fc2 = F.relu(_fc2)\n",
    "        \n",
    "        _fc3 = self.bn3(self.fc3(_fc2))\n",
    "        _fc3 = F.relu(_fc3)\n",
    "        \n",
    "        _fc4 = self.fc4(_fc3)\n",
    "        output = F.softmax(_fc4, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, class_num,seq_len=10, initrange=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input = input_dim\n",
    "        self.hidden_size = 256\n",
    "        self.rnn = nn.LSTM(self.input, self.hidden_size, num_layers=5, dropout=0.5 , batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_size*seq_len,512)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(512)\n",
    "        self.fc2 = torch.nn.Linear(512,160)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(160)\n",
    "        #self.fc3 = torch.nn.Linear(160,80)\n",
    "        #self.bn3 = torch.nn.BatchNorm1d(80)\n",
    "        self.fc4 = torch.nn.Linear(160, class_num)\n",
    "        self.init_weights(initrange)\n",
    "        self.class_num = class_num\n",
    "    \n",
    "    def init_weights(self, initrange):\n",
    "        \n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "        #self.fc3.weight.data.uniform_(-initrange, initrange)\n",
    "        #self.fc3.bias.data.zero_()\n",
    "        self.fc4.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc4.bias.data.zero_()\n",
    "    \n",
    "    \n",
    "    def forward(self, _input, batch_size):\n",
    "        \n",
    "        h_cell = torch.randn(5, batch_size, self.hidden_size, dtype=torch.float).to(device)\n",
    "        c_cell = torch.randn(5, batch_size, self.hidden_size, dtype=torch.float).to(device)\n",
    "        rnn_feat, (newh_cell, newc_cell) = self.rnn(_input, (h_cell, c_cell))\n",
    "        rnn_feat = rnn_feat.reshape((-1,rnn_feat.shape[1]*rnn_feat.shape[2]))\n",
    "        \n",
    "        _fc1 =self.bn1(self.fc1(rnn_feat))\n",
    "        #_fc1 = F.dropout(F.relu(_fc1), p=0.3)\n",
    "        _fc1 = F.relu(_fc1)\n",
    "        \n",
    "        _fc2 = self.bn2(self.fc2(_fc1))\n",
    "        _fc2 = F.relu(_fc2)\n",
    "        \n",
    "        #_fc3 = self.bn3(self.fc3(_fc2))\n",
    "        #_fc3 = F.relu(_fc3)\n",
    "        \n",
    "        _fc4 = self.fc4(_fc2)\n",
    "        output = F.softmax(_fc4, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_Single(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, class_num, initrange=0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, 128)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(128)\n",
    "        self.fc2 = torch.nn.Linear(128, 64)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
    "        self.fc3 = torch.nn.Linear(64,16)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(16)\n",
    "        self.fc4 = torch.nn.Linear(16, class_num)\n",
    "        self.class_num = class_num\n",
    "        self.init_weights(initrange)\n",
    "    \n",
    "    def init_weights(self, initrange):\n",
    "        \n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "        self.fc3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc3.bias.data.zero_()\n",
    "        self.fc4.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc4.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \n",
    "        _fc1 = F.relu(self.fc1(_input))\n",
    "        \n",
    "        _bn1 = self.bn1(_fc1)\n",
    "        \n",
    "        _fc2 = F.relu(self.fc2(_bn1))\n",
    "        _bn2 = self.bn2(_fc2)\n",
    "        \n",
    "        _fc3 = F.relu(self.fc3(_bn2))\n",
    "        _bn3 = self.bn3(_fc3)\n",
    "        \n",
    "        _fc4 = self.fc4(_bn3)\n",
    "        \n",
    "        output = F.softmax(_fc4, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(reshape=False, seq_len=10, bs=8, n_test = 0.35, n_valid=0.2):\n",
    "    \n",
    "    # load pose dataset\n",
    "    dataset = SinglePoseDataset(n_frames=seq_len)\n",
    "    targets = dataset.y\n",
    "    \n",
    "    # reshape from N,10,34 to N,340\n",
    "    if reshape:\n",
    "        dataset.reshape_features()\n",
    "    \n",
    "    # calculate split size\n",
    "    #train,  valid, test = dataset.get_splits()\n",
    "    \n",
    "    #stratified train test split\n",
    "    train_idx, valid_idx = train_test_split(np.arange(len(targets)), test_size=0.35, stratify=targets)\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    train_dl  = DataLoader(dataset, batch_size=bs, sampler=train_sampler, drop_last=True)\n",
    "    valid_dl  = DataLoader(dataset, batch_size=512, sampler=valid_sampler, drop_last=True)\n",
    "    \n",
    "    # prepare data loaders\n",
    "    #train_dl =  DataLoader(train, batch_size=bs, shuffle=True, drop_last=True)\n",
    "    #valid_dl  = DataLoader(valid, batch_size=512, shuffle=False, drop_last=True)\n",
    "    #test_dl  =  DataLoader(test, batch_size=1024, shuffle=False, drop_last=True)\n",
    "    \n",
    "    return {'train':train_dl, 'valid':valid_dl}, {'train':len(train_sampler), 'valid':len(valid_sampler)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, loss, acc, epoch, save_path):\n",
    "    \n",
    "    base_dir = os.path.basename(os.getcwd())\n",
    "    \n",
    "    if base_dir =='train':\n",
    "        parent_dir = os.path.dirname(os.getcwd())\n",
    "        os.chdir(parent_dir)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "    \n",
    "    print('SAVING EPOCH %d'%epoch)\n",
    "    filename = 'epoch_%d'%epoch + '_loss_%f.pth'%loss\n",
    "    SAVE_FILE = os.path.join(save_path,filename)\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            'acc':  acc,\n",
    "            }, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, dataset_sizes, num_epochs=3000):\n",
    "    \n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    #get train and val dataloaders\n",
    "    #dataloaders, dataset_sizes = prepare_data()\n",
    "\n",
    "    # define the optimization\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.1)\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=5, factor=0.1,verbose=True)\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = -1\n",
    "    conf_train = 0.0\n",
    "    conf_valid = 0.0\n",
    "    # enumerate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs-1))\n",
    "        print('-'*10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train() # St model to training mode\n",
    "            \n",
    "            else:\n",
    "                model.eval() # Set model to evaluate mode\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            # Initialize the prediction and label lists(tensors)\n",
    "            pred_tensor  = torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "            class_tensor = torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "            \n",
    "            # enumerate over mini_batch\n",
    "            for i, (inputs, targets)  in enumerate(dataloaders[phase]):\n",
    "                inputs  = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                # clear the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train pahse\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    \n",
    "                    # compute model outputs\n",
    "                    outputs = model(inputs, inputs.shape[0])\n",
    "                    \n",
    "                    # calculate outputs\n",
    "                    _, preds = torch.max(outputs, dim=1)\n",
    "                    \n",
    "                    # calculate the loss\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    # backward + optimize only ig in training phase\n",
    "                    if phase == 'train':\n",
    "                        # calculate gradient\n",
    "                        loss.backward()\n",
    "                    \n",
    "                        # update model weights\n",
    "                        optimizer.step()\n",
    "                \n",
    "                #statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds==targets)\n",
    "                \n",
    "                # Append batch prediction results\n",
    "                pred_tensor = torch.cat([pred_tensor,preds.view(-1).cpu()])\n",
    "                class_tensor  = torch.cat([class_tensor,targets.view(-1).cpu()])\n",
    "                \n",
    "            \n",
    "            #epoch loss and accuracy\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc  = running_corrects.double() / dataset_sizes[phase]\n",
    "            history[phase].append((epoch_loss, epoch_acc, ))\n",
    "            \n",
    "            if phase=='valid':\n",
    "                scheduler.step()\n",
    "\n",
    "            # Confusion matrix\n",
    "            conf_mat = confusion_matrix(class_tensor.numpy(), pred_tensor.numpy())\n",
    "            # Per-class accuracy\n",
    "            #per_class_accuracy = np.round(100*conf_mat.diagonal()/conf_mat.sum(1),4)\n",
    "            #Precision, Recall, F1_Score\n",
    "            precision = precision_score(class_tensor.numpy(), pred_tensor.numpy(), average=None)\n",
    "            recall = recall_score(class_tensor.numpy(), pred_tensor.numpy(), average=None)\n",
    "            f_score = f1_score(class_tensor.numpy(), pred_tensor.numpy(), average=None)\n",
    "            \n",
    "            print('{} : Loss: {:.4f}, Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            #print('{} : Confusion Matrix: {}'.format(phase, conf_mat))\n",
    "            #print('{} : Precision per class: {}'.format(phase, np.round(precision,4)))\n",
    "            #print('{} : Recall per class: {}'.format(phase, np.round(recall,4)))\n",
    "            print('{} : F1_Score per class: {}'.format(phase, np.round(f_score,4)))\n",
    "            print()\n",
    "            \n",
    "            if phase== 'valid' and loss_< epoch_loss:\n",
    "                epoch_ = epoch\n",
    "                loss_  = epoch_loss\n",
    "                conf_valid = conf_mat\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                save_model(model, optimizer, loss_, epoch_acc, epoch_, save_path=r'checkpoints_fallmodel/act_fclstm_15')\n",
    "            \n",
    "            if phase== 'train' and epoch_acc>best_acc:\n",
    "                conf_train = conf_mat\n",
    "               \n",
    "                \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    \n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed //60, time_elapsed %60))\n",
    "    print('Best val Acc: {:4f}'.format(epoch_acc))\n",
    "    \n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    #save_model(model, optimizer, epoch_loss, best_acc, epoch, save_path=r'checkpoints_fallmodel/act_fclstm_15')\n",
    "    \n",
    "    return history, model, conf_train, conf_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN_model = RNNModel(input_dim=24, class_num=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test dataloaders\n",
    "#dataloaders, dataset_sizes = prepare_data(batch_size=16)\n",
    "#history, model, conf_train, conf_valid = train_model(RNN_model, dataloaders, dataset_sizes,num_epochs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU_model = GRUModel(input_dim=24, class_num=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test dataloaders\n",
    "#dataloaders, dataset_sizes = prepare_data(batch_size=16)\n",
    "#history, model, conf_train, conf_valid = train_model(GRU_model, dataloaders, dataset_sizes,num_epochs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = LSTMModel(input_dim=24, class_num=3, seq_len=15).to(device)\n",
    "total_params = sum(p.numel() for p in LSTM_model.parameters() if p.requires_grad)\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test dataloaders\n",
    "dataloaders, dataset_sizes = prepare_data(bs=512, seq_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history, model, conf_train, conf_valid = train_model(LSTM_model, dataloaders, dataset_sizes,num_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc  = []\n",
    "train_loss = []\n",
    "val_acc =  []\n",
    "val_loss = []\n",
    "\n",
    "for train_item, val_item in zip(history['train'],history['valid']):\n",
    "    \n",
    "    val_loss.append(val_item.__getitem__(0))\n",
    "    val_acc.append(val_item.__getitem__(1).cpu().detach().numpy())\n",
    "    \n",
    "    train_loss.append(train_item.__getitem__(0))\n",
    "    train_acc.append(train_item.__getitem__(1).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'legend.fontsize': 'x-large',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=range(2000)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(num_epochs, train_acc, label='Training Accuracy')\n",
    "plt.plot(num_epochs, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy', fontsize=18)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('plots/lstm_seq10_acc.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=range(2000)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(num_epochs, train_loss, label='Training Loss')\n",
    "plt.plot(num_epochs, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('plots/lstm_seq10_loss.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the size of figure 542129345\n",
    "plt.figure(figsize=(8,8))\n",
    "#normalize each column (class) with total datapoints in that column  \n",
    "conf_train = conf_train.astype('float')/conf_train.sum(axis=1)*100\n",
    "#plot confusion matrix \n",
    "p=sns.heatmap(conf_train, xticklabels=['Fall','Stand','Tie'], yticklabels=['Fall','Stand','Tie'],\n",
    "              cbar=False, annot=True, cmap='coolwarm',robust=True, fmt='.1f',annot_kws={'size':20})\n",
    "plt.title('Training matrix: Actual labels Vs Predicted labels')\n",
    "plt.savefig('plots/lstm_seq10_train_cf.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the size of figure \n",
    "plt.figure(figsize=(8,8))\n",
    "#normalize each column (class) with total datapoints in that column  \n",
    "conf_valid = conf_valid.astype('float')/conf_valid.sum(axis=1)*100\n",
    "#plot confusion matrix \n",
    "p=sns.heatmap(conf_valid, xticklabels=['Fall','Stand','Tie'], yticklabels=['Fall','Stand','Tie'],\n",
    "              cbar=False, annot=True, cmap='coolwarm',robust=True, fmt='.1f',annot_kws={'size':20})\n",
    "plt.title('Validation matrix: Actual labels vs Predicted labels')\n",
    "plt.savefig('plots/lstm_seq10_valid_cf.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloaders, dataset_sizes):\n",
    "    #defaultdict to save list in dict\n",
    "    history = defaultdict(list)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    model.eval() # Set model to evaluate mode\n",
    "    phase = 'test'            \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    # Initialize the prediction and label lists(tensors)\n",
    "    pred_tensor  = torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    class_tensor = torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "    # enumerate over mini_batch\n",
    "    for i, (inputs, targets)  in enumerate(dataloaders['test']):\n",
    "        inputs  = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # compute model outputs\n",
    "        outputs = model(inputs, inputs.shape[0])\n",
    "\n",
    "        # calculate outputs\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        #statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds==targets)\n",
    "\n",
    "        # Append batch prediction results\n",
    "        pred_tensor = torch.cat([pred_tensor,preds.view(-1).cpu()])\n",
    "        class_tensor  = torch.cat([class_tensor,targets.view(-1).cpu()])\n",
    "                \n",
    "    #epoch loss and accuracy\n",
    "    loss = running_loss / dataset_sizes['test'].__len__()\n",
    "    acc  = running_corrects.double() / dataset_sizes['test'].__len__()\n",
    "    history['test'].append((loss,acc))\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_mat = confusion_matrix(class_tensor.numpy(), pred_tensor.numpy())\n",
    "    # Per-class accuracy\n",
    "    per_class_accuracy = np.round(100*conf_mat.diagonal()/conf_mat.sum(1),4)\n",
    "    #Precision, Recall, F1_Score\n",
    "    precision = precision_score(class_tensor.numpy(), pred_tensor.numpy(), average='micro')\n",
    "    recall = recall_score(class_tensor.numpy(), pred_tensor.numpy(), average='micro')\n",
    "    f_score = f1_score(class_tensor.numpy(), pred_tensor.numpy(), average='micro')\n",
    "\n",
    "    #print('{} : Loss: {:.4f}, Acc: {:.4f}'.format(phase, loss, acc))\n",
    "    #print('{} : Confusion Matrix: {}, Per_class_accuracy: {}'.format(phase, conf_mat, per_class_accuracy))\n",
    "    print('{} : Precision: {:.4f}, Recall: {:.4f}, F1_Score: {:.4f}'.format(phase, precision, recall, f_score))\n",
    "    print()\n",
    "            \n",
    "    #set the size of figure \n",
    "    plt.figure(figsize=(8,8))\n",
    "    #normalize each column (class) with total datapoints in that column  \n",
    "    conf_mat = conf_mat.astype('float')/conf_mat.sum(axis=1)*100\n",
    "    #plot confusion matrix \n",
    "    p=sns.heatmap(conf_mat, xticklabels=['Fall','Stand','Tie'], yticklabels=['Fall','Stand','Tie'],\n",
    "              cbar=False, annot=True, cmap='coolwarm',robust=True, fmt='.1f',annot_kws={'size':20})\n",
    "    plt.title('Test matrix: Actual labels Vs Predicted labels')\n",
    "    plt.savefig('plots/lstm_seq10_test_cf.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, dataloaders, dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:alphapose] *",
   "language": "python",
   "name": "conda-env-alphapose-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
